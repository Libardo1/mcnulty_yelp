{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English ##Note you'll need to install Spacy and download its dependencies\n",
    "parser = English()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    text = re.sub('[^a-zA-Z ]','',text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "#     text = str(TextBlob(text).correct())\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3),min_df=3, max_features=3000,tokenizer=tokenizeText)\n",
    "tfvectorizer = TfidfVectorizer(ngram_range=(1,3),min_df = 3,max_features=3000,tokenizer=tokenizeText,sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take note of the assumptions made in the vectorizer specifications. There are two kinds of vectorizers initialized: count and tfidf. I've limited both to consider only n-grams that appear at least ten times. I've also limited the feature set into the top 3,000 n-grams that appear the most often in the reviews. Also it only extracts unigrams to trigrams. You can edit any of the parameters.\n",
    "\n",
    "### You can switch between count and tfidf vectorizers by changing between \"vectorizer\" and \"tfvectorizer\" in one of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "with open(\"C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/review_business_df.pkl\", 'rb') as picklefile: \n",
    "    review_business_df = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_df = review_business_df.sample(10000,random_state=1).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that I've only extract 10,000 sample reviews. This is just to manage its tracktability. Lemmatizing takes a really long time. Indirectly, 10,000 reviews is also big enough relative to the 1,000 features (discussed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_df = sample_df.iloc[:,0:2]\n",
    "short_df.text= short_df.text.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Gets the count of each word in each sentence\n",
    "countfeature = vectorizer.fit_transform(short_df.text)\n",
    "tffeature = tfvectorizer.fit_transform(short_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Turns count/tfidf matrix into a dataframe\n",
    "countfeaturedf = pd.DataFrame(countfeature.A, columns=vectorizer.get_feature_names())\n",
    "tffeaturedf = pd.DataFrame(tffeature.A, columns=tfvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Concat Y and X\n",
    "new_df_count = pd.concat((short_df,countfeaturedf),axis=1)\n",
    "new_df_tf = pd.concat((short_df,tffeaturedf),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 3002 entries, text to zucchini\n",
      "dtypes: int64(3001), object(1)\n",
      "memory usage: 229.0+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 3002 entries, text to zucchini\n",
      "dtypes: float64(3000), int64(1), object(1)\n",
      "memory usage: 229.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(new_df_count.info())\n",
    "print(new_df_tf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Do not run until the above cells are run\n",
    "import pickle\n",
    "new_df_count.to_pickle('new_df_count.pkl')\n",
    "new_df_tf.to_pickle('new_df_tf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "new_df_count = pd.read_pickle(\"new_df_count.pkl\")\n",
    "new_df_tf = pd.read_pickle('new_df_tf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string(x):\n",
    "    if x == 5:\n",
    "        return \"five\"\n",
    "    elif x == 4:\n",
    "        return 'four'\n",
    "    elif x == 3:\n",
    "        return 'three'\n",
    "    elif x == 2:\n",
    "        return 'two'\n",
    "    elif x == 1:\n",
    "        return 'one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df_count['stars_x'] = new_df_count['stars_x'].apply(string)\n",
    "new_df_tf['stars_x'] = new_df_tf['stars_x'].apply(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collapse_df_count = new_df_count.copy()\n",
    "collapse_df_tf = new_df_tf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Apply this if you want to collapse the 5-star ratings by removing 3-star reviews and combining 1 & 2 stars \n",
    "##and 4 & 5 ratings\n",
    "\n",
    "def collapse(x):\n",
    "    if x == 'four' or x == 'five':\n",
    "        return 'four/five'\n",
    "    elif x == 'one' or x == 'two':\n",
    "        return 'one/two'\n",
    "    elif x == 'three':\n",
    "        return 'three'\n",
    "\n",
    "# Run if you want to delete 3-stars\n",
    "# collapse_df = collapse_df[new_df['stars_x'] != 'three']    \n",
    "    \n",
    "collapse_df_count.stars_x = new_df_count.stars_x.apply(collapse)\n",
    "collapse_df_tf.stars_x = new_df_tf.stars_x.apply(collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "four/five    0.6553\n",
       "one/two      0.1953\n",
       "three        0.1494\n",
       "Name: stars_x, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapse_df_count.stars_x.value_counts()/collapse_df_count.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Adds sentiment as a feature. Note that I added 1 because some algorithms won't accept negative sentiment scores\n",
    "##Sentiment scores is based on TextBlob where it goes from -1.0 to 1.0 (negative to positive)\n",
    "##Change the reference DF if you want to go back to using the 5-star categories\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "collapse_df_count['senti'] = collapse_df_count['text'].apply(lambda x: TextBlob(x).sentiment[0] + 1)\n",
    "collapse_df_tf['senti'] = collapse_df_tf['text'].apply(lambda x: TextBlob(x).sentiment[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Split into train and test at 75/25\n",
    "from sklearn.cross_validation import train_test_split \n",
    "train, test = train_test_split(collapse_df_count.values,test_size = 0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Split X & Y\n",
    "X_train = train[:,2:]\n",
    "Y_train = train[:,1]\n",
    "X_test = test[:,2:]\n",
    "Y_test = test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy:  0.6052\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.74      0.76      0.75      1646\n",
      "    one/two       0.40      0.44      0.42       478\n",
      "      three       0.19      0.14      0.16       376\n",
      "\n",
      "avg / total       0.59      0.61      0.60      2500\n",
      "\n",
      "[ 0.62968516  0.60769615  0.60969515  0.5967984   0.61311311]\n"
     ]
    }
   ],
   "source": [
    "## KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1).fit(X_train,Y_train)\n",
    "knn_Y_pred = knn.predict(X_test)\n",
    "print(\"KNN Accuracy: \",np.mean(knn_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,knn_Y_pred))\n",
    "print(cross_val_score(knn,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy:  0.7652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.87      0.88      0.87      1646\n",
      "    one/two       0.70      0.68      0.69       478\n",
      "      three       0.40      0.39      0.39       376\n",
      "\n",
      "avg / total       0.76      0.77      0.76      2500\n",
      "\n",
      "[ 0.77311344  0.77111444  0.76261869  0.76688344  0.75375375]\n"
     ]
    }
   ],
   "source": [
    "## NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB().fit(X_train,Y_train)\n",
    "nb_Y_pred = nb.predict(X_test)\n",
    "print(\"NB Accuracy: \",np.mean(nb_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,nb_Y_pred))\n",
    "print(cross_val_score(nb,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistc Accuracy:  0.7764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.84      0.92      0.88      1646\n",
      "    one/two       0.73      0.67      0.70       478\n",
      "      three       0.40      0.29      0.34       376\n",
      "\n",
      "avg / total       0.76      0.78      0.76      2500\n",
      "\n",
      "[ 0.77561219  0.77761119  0.77361319  0.7813907   0.76476476]\n"
     ]
    }
   ],
   "source": [
    "## Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression().fit(X_train,Y_train)\n",
    "log_Y_pred = log.predict(X_test)\n",
    "print(\"Logistc Accuracy: \",np.mean(log_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,log_Y_pred))\n",
    "print(cross_val_score(log,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistc Accuracy:  0.7556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.86      0.87      0.87      1646\n",
      "    one/two       0.70      0.67      0.69       478\n",
      "      three       0.35      0.35      0.35       376\n",
      "\n",
      "avg / total       0.75      0.76      0.75      2500\n",
      "\n",
      "[ 0.76111944  0.76011994  0.75762119  0.77188594  0.75425425]\n"
     ]
    }
   ],
   "source": [
    "## Logistic with balanced weights?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logb = LogisticRegression(class_weight='balanced').fit(X_train,Y_train)\n",
    "logb_Y_pred = logb.predict(X_test)\n",
    "print(\"Logistc Accuracy: \",np.mean(logb_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,logb_Y_pred))\n",
    "print(cross_val_score(logb,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Linear Accuracy:  0.732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.84      0.87      0.85      1646\n",
      "    one/two       0.68      0.61      0.64       478\n",
      "      three       0.30      0.29      0.29       376\n",
      "\n",
      "avg / total       0.73      0.73      0.73      2500\n",
      "\n",
      "[ 0.74162919  0.74262869  0.73763118  0.75287644  0.73173173]\n",
      "[ 0.75912044  0.75512244  0.75112444  0.75387694  0.74124124]\n"
     ]
    }
   ],
   "source": [
    "## Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "svcl = SVC(kernel='linear').fit(X_train,Y_train)\n",
    "svcl_Y_pred = svcl.predict(X_test)\n",
    "print(\"SVC Linear Accuracy: \",np.mean(svcl_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,svcl_Y_pred))\n",
    "print(cross_val_score(svcl,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))\n",
    "print(cross_val_score(svcl,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## RBF SVC\n",
    "from sklearn.svm import SVC\n",
    "svcrbf = SVC(kernel='rbf', gamma=1).fit(X_train,Y_train)\n",
    "svcrbf_Y_pred = svcrbf.predict(X_test)\n",
    "print(\"SVC RBF Accuracy: \",np.mean(svcrbf_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,svcrbf_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "dt_Y_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \",np.mean(dt_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,dt_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests Accuracy:  0.7408\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.77      0.95      0.85      1646\n",
      "    one/two       0.69      0.54      0.60       478\n",
      "      three       0.38      0.09      0.15       376\n",
      "\n",
      "avg / total       0.69      0.74      0.70      2500\n",
      "\n",
      "[ 0.76261869  0.74212894  0.73463268  0.75437719  0.74774775]\n"
     ]
    }
   ],
   "source": [
    "## Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train,Y_train)\n",
    "rf_Y_pred = rf.predict(X_test)\n",
    "print(\"Random Forests Accuracy: \",np.mean(rf_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,rf_Y_pred))\n",
    "print(cross_val_score(rf,collapse_df_count.iloc[:,2:].values,collapse_df_count.iloc[:,1].values,cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read twitter feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = pd.read_pickle('C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/tweets_clean.pkl')\n",
    "tweet_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 803 entries, 0 to 802\n",
      "Data columns (total 6 columns):\n",
      "business      803 non-null object\n",
      "created_at    803 non-null object\n",
      "lang          803 non-null object\n",
      "location      527 non-null object\n",
      "user_id       803 non-null int64\n",
      "text          803 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 37.7+ KB\n"
     ]
    }
   ],
   "source": [
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-87829de8eeb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweet_sparse_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "tweet_sparse_matrix = vectorizer.transform(tweet_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_count_df = pd.DataFrame(tweet_sparse_matrix.A, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweet_count_df.to_pickle(\"tweet_count_df.pkl\")\n",
    "tweet_count_df = pd.read_pickle(\"tweet_count_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweet_df2 = pd.concat((tweet_df,tweet_count_df),axis=1)\n",
    "tweet_df2['senti'] = tweet_df.text.apply(lambda x: TextBlob(x).sentiment[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tweet = tweet_df2.iloc[:,6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Naive Bayes\n",
    "tweet_pred = nb.predict(X_tweet)\n",
    "tweet_pred_prob = nb.predict_proba(X_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df['pred'] = tweet_pred\n",
    "\n",
    "tweet_prob_df = pd.DataFrame(tweet_pred_prob,columns = ['prob_4/5','prob_1/2','prob_3'])\n",
    "tweet_df = pd.concat((tweet_df,tweet_prob_df),axis =1)\n",
    "tweet_df = tweet_df.drop_duplicates('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df.to_pickle(\"tweet_df_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweet_df_pred = pd.read_pickle(\"tweet_df_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df_pred.to_csv(\"tweet_df_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
