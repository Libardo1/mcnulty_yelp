{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English ##Note you'll need to install Spacy and download its dependencies\n",
    "parser = English()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    text = re.sub('[^a-zA-Z ]','',text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "#     text = str(TextBlob(text).correct())\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3),min_df=3, max_features=3000,tokenizer=tokenizeText)\n",
    "tfvectorizer = TfidfVectorizer(ngram_range=(1,3),min_df = 3,max_features=3000,tokenizer=tokenizeText,sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take note of the assumptions made in the vectorizer specifications. There are two kinds of vectorizers initialized: count and tfidf. I've limited both to consider only n-grams that appear at least ten times. I've also limited the feature set into the top 1,000 n-grams that appear the most often in the reviews. Also it only extracts unigrams to trigrams. You can edit any of the parameters.\n",
    "\n",
    "### You can switch between count and tfidf vectorizers by changing between \"vectorizer\" and \"tfvectorizer\" in one of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "with open(\"C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/review_business_df.pkl\", 'rb') as picklefile: \n",
    "    review_business_df = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_df = review_business_df.sample(10000,random_state=1).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that I've only extract 10,000 sample reviews. This is just to manage its tracktability. Lemmatizing takes a really long time. Indirectly, 10,000 reviews is also big enough relative to the 1,000 features (discussed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_df = sample_df.iloc[:,0:2]\n",
    "short_df.text= short_df.text.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Gets the count of each word in each sentence\n",
    "countfeature = vectorizer.fit_transform(short_df.text)\n",
    "tffeature = tfvectorizer.fit_transform(short_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Turns count/tfidf matrix into a dataframe\n",
    "countfeaturedf = pd.DataFrame(countfeature.A, columns=vectorizer.get_feature_names())\n",
    "tffeaturedf = pd.DataFrame(tffeature.A, columns=tfvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Concat Y and X\n",
    "new_df_count = pd.concat((short_df,countfeaturedf),axis=1)\n",
    "new_df_tf = pd.concat((short_df,tffeaturedf),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 3002 entries, text to zucchini\n",
      "dtypes: int64(3001), object(1)\n",
      "memory usage: 229.0+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 3002 entries, text to zucchini\n",
      "dtypes: float64(3000), int64(1), object(1)\n",
      "memory usage: 229.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(new_df_count.info())\n",
    "print(new_df_tf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "new_df_count.to_pickle('new_df_count.pkl')\n",
    "new_df_tf.to_pickle('new_df_tf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "new_df_count = pd.read_pickle(\"new_df_count.pkl\")\n",
    "new_df_tf = pd.read_pickle('new_df_tf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string(x):\n",
    "    if x == 5:\n",
    "        return \"five\"\n",
    "    elif x == 4:\n",
    "        return 'four'\n",
    "    elif x == 3:\n",
    "        return 'three'\n",
    "    elif x == 2:\n",
    "        return 'two'\n",
    "    elif x == 1:\n",
    "        return 'one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df_count['stars_x'] = new_df_count['stars_x'].apply(string)\n",
    "new_df_tf['stars_x'] = new_df_tf['stars_x'].apply(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collapse_df_count = new_df_count.copy()\n",
    "collapse_df_tf = new_df_tf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Apply this if you want to collapse the 5-star ratings by removing 3-star reviews and combining 1 & 2 stars \n",
    "##and 4 & 5 ratings\n",
    "\n",
    "def collapse(x):\n",
    "    if x == 'four' or x == 'five':\n",
    "        return 'four/five'\n",
    "    elif x == 'one' or x == 'two':\n",
    "        return 'one/two'\n",
    "    elif x == 'three':\n",
    "        return 'three'\n",
    "\n",
    "collapse_df_count.stars_x = new_df_count.stars_x.apply(collapse)\n",
    "collapse_df_tf.stars_x = new_df_tf.stars_x.apply(collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run if you want to delete 3-stars\n",
    "# collapse_df = collapse_df[new_df['stars_x'] != 'three']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Adds sentiment as a feature. Note that I added 1 because some algorithms won't accept negative sentiment scores\n",
    "##Sentiment scores is based on TextBlob where it goes from -1.0 to 1.0 (negative to positive)\n",
    "##Change the reference DF if you want to go back to using the 5-star categories\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "collapse_df_count['senti'] = collapse_df_count['text'].apply(lambda x: TextBlob(x).sentiment[0] + 1)\n",
    "collapse_df_tf['senti'] = collapse_df_tf['text'].apply(lambda x: TextBlob(x).sentiment[0] + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Split into train and test at 75/25\n",
    "from sklearn.cross_validation import train_test_split \n",
    "train, test = train_test_split(collapse_df_count.values,test_size = 0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Split X & Y\n",
    "X_train = train[:,2:]\n",
    "Y_train = train[:,1]\n",
    "X_test = test[:,2:]\n",
    "Y_test = test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy:  0.6052\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.74      0.76      0.75      1646\n",
      "    one/two       0.40      0.44      0.42       478\n",
      "      three       0.19      0.14      0.16       376\n",
      "\n",
      "avg / total       0.59      0.61      0.60      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1).fit(X_train,Y_train)\n",
    "knn_Y_pred = knn.predict(X_test)\n",
    "print(\"KNN Accuracy: \",np.mean(knn_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,knn_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy:  0.7652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.87      0.88      0.87      1646\n",
      "    one/two       0.70      0.68      0.69       478\n",
      "      three       0.40      0.39      0.39       376\n",
      "\n",
      "avg / total       0.76      0.77      0.76      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB().fit(X_train,Y_train)\n",
    "nb_Y_pred = nb.predict(X_test)\n",
    "print(\"NB Accuracy: \",np.mean(nb_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,nb_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistc Accuracy:  0.7764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  four/five       0.84      0.92      0.88      1646\n",
      "    one/two       0.73      0.67      0.70       478\n",
      "      three       0.40      0.29      0.34       376\n",
      "\n",
      "avg / total       0.76      0.78      0.76      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression().fit(X_train,Y_train)\n",
    "log_Y_pred = log.predict(X_test)\n",
    "print(\"Logistc Accuracy: \",np.mean(log_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,log_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "svcl = SVC(kernel='linear').fit(X_train,Y_train)\n",
    "svcl_Y_pred = svcl.predict(X_test)\n",
    "print(\"SVC Linear Accuracy: \",np.mean(svcl_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,svcl_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## RBF SVC\n",
    "from sklearn.svm import SVC\n",
    "svcrbf = SVC(kernel='rbf', gamma=1).fit(X_train,Y_train)\n",
    "svcrbf_Y_pred = svcrbf.predict(X_test)\n",
    "print(\"SVC RBF Accuracy: \",np.mean(svcrbf_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,svcrbf_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "dt_Y_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \",np.mean(dt_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,dt_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train,Y_train)\n",
    "rf_Y_pred = rf.predict(X_test)\n",
    "print(\"Random Forests Accuracy: \",np.mean(rf_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,rf_Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read twitter feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = pd.read_pickle('C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/tweets_clean.pkl')\n",
    "tweet_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 803 entries, 0 to 802\n",
      "Data columns (total 6 columns):\n",
      "business      803 non-null object\n",
      "created_at    803 non-null object\n",
      "lang          803 non-null object\n",
      "location      527 non-null object\n",
      "user_id       803 non-null int64\n",
      "text          803 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 37.7+ KB\n"
     ]
    }
   ],
   "source": [
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sparse_matrix = vectorizer.transform(tweet_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_count_df = pd.DataFrame(tweet_sparse_matrix.A, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweet_df2 = pd.concat((tweet_df,tweet_count_df),axis=1)\n",
    "tweet_df2['senti'] = tweet_df.text.apply(lambda x: TextBlob(x).sentiment[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tweet = tweet_df2.iloc[:,6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Naive Bayes\n",
    "tweet_pred = nb.predict(X_tweet)\n",
    "tweet_pred_prob = nb.predict_proba(X_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df['pred'] = tweet_pred\n",
    "\n",
    "tweet_prob_df = pd.DataFrame(tweet_pred_prob,columns = ['prob_4/5','prob_1/2','prob_3'])\n",
    "tweet_df = pd.concat((tweet_df,tweet_prob_df),axis =1)\n",
    "tweet_df = tweet_df.drop_duplicates('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df.to_pickle(\"tweet_df_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweet_df = pd.read_pickle(\"tweet_df_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob_4/5</th>\n",
       "      <th>prob_1/2</th>\n",
       "      <th>prob_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Too bad  managers were told not to receive our...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.989221</td>\n",
       "      <td>0.007199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>you guys should send me a lifetime supply as ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.277998</td>\n",
       "      <td>0.642817</td>\n",
       "      <td>0.079185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Paying workers with pre paid debit cards?</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.051944</td>\n",
       "      <td>0.837278</td>\n",
       "      <td>0.110778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Not nice of u  to charge us for playing games ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.553386</td>\n",
       "      <td>0.305624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>.’s giant portions of pasta, lasagna &amp;amp; chi...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.256460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Oh  your customer service is disappointing on ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>0.879715</td>\n",
       "      <td>0.005469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>I got charged for a salad for eating a bite o...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.098855</td>\n",
       "      <td>0.773735</td>\n",
       "      <td>0.127410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>nasty ass place I just found a lady bug in my...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.986581</td>\n",
       "      <td>0.012133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Suggestion for the  kid's menu: Maybe remove ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.158861</td>\n",
       "      <td>0.797564</td>\n",
       "      <td>0.043575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Hands down worst service ever..</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.039142</td>\n",
       "      <td>0.905843</td>\n",
       "      <td>0.055015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>there's something gross in my food😁</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.273510</td>\n",
       "      <td>0.596422</td>\n",
       "      <td>0.130068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>When you order online for an 11:00 pickup and...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.037292</td>\n",
       "      <td>0.946924</td>\n",
       "      <td>0.015784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>table games are phenomenal, but unannounced f...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.426427</td>\n",
       "      <td>0.538117</td>\n",
       "      <td>0.035457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>eating at Carencro Louisiana location. So col...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.950593</td>\n",
       "      <td>0.043313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>The  by Children's Mercy Park may be the worst...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.359598</td>\n",
       "      <td>0.549990</td>\n",
       "      <td>0.090412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>and we waited 13 minutes to get our drink ord...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.986108</td>\n",
       "      <td>0.013244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>worst service ever 🙃</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.045985</td>\n",
       "      <td>0.893782</td>\n",
       "      <td>0.060233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>too bad none of my friends want to come with!</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.223874</td>\n",
       "      <td>0.696361</td>\n",
       "      <td>0.079765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>When you cant decide what you want at  so you ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.277673</td>\n",
       "      <td>0.536540</td>\n",
       "      <td>0.185787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Was eating left over  &amp;amp; Hops jumped up on ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.152213</td>\n",
       "      <td>0.683653</td>\n",
       "      <td>0.164134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>\\n piece hot and pieces frozen. .🙈❤\\nThis is ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.206504</td>\n",
       "      <td>0.650939</td>\n",
       "      <td>0.142557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Terrible experience at Chili's Las Vegas airp...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.990475</td>\n",
       "      <td>0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>your staff thought this would be an acceptabl...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.164441</td>\n",
       "      <td>0.604505</td>\n",
       "      <td>0.231054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>oops I forgot to finish my rant, anyways they...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.399334</td>\n",
       "      <td>0.497711</td>\n",
       "      <td>0.102955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Had to contact red lobster about that horrible...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.967299</td>\n",
       "      <td>0.031208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>your biscuits suck, your decor sucks, the soap...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>0.883840</td>\n",
       "      <td>0.098844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>I just entered a  for a Gift Card to</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.316505</td>\n",
       "      <td>0.522541</td>\n",
       "      <td>0.160954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Whenever you want to make me a partner  just l...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.416631</td>\n",
       "      <td>0.572696</td>\n",
       "      <td>0.010673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Arrive at  for lunch. Handed a pager, 5-10 min...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.110134</td>\n",
       "      <td>0.866937</td>\n",
       "      <td>0.022929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Its happened again, Its happened again, Kevin ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.311300</td>\n",
       "      <td>0.631659</td>\n",
       "      <td>0.057041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>has touch screens that each individual can pa...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.114376</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.212624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>why do you give us 3  for a party of 2. You k...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.192659</td>\n",
       "      <td>0.551770</td>\n",
       "      <td>0.255571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Was going to make snarky comment about , but b...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.281610</td>\n",
       "      <td>0.601295</td>\n",
       "      <td>0.117095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>stays messing up to-go orders</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.192192</td>\n",
       "      <td>0.746653</td>\n",
       "      <td>0.061155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Beyond angry😡.Not only do i have to wait to ge...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.618964</td>\n",
       "      <td>0.314903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>i ordered enchiladas with specific sides. Whe...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.568754</td>\n",
       "      <td>0.185846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>so disappointed in you guys! 💔 you've never m...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.995637</td>\n",
       "      <td>0.003728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>YO I FOUND 44 NASTY SPIDERS IN UR RESTAURANT ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.986109</td>\n",
       "      <td>0.010405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>the typa restaurant to air amy schumers stand...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.396477</td>\n",
       "      <td>0.462611</td>\n",
       "      <td>0.140912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>who in charge of the aux in</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.309455</td>\n",
       "      <td>0.513951</td>\n",
       "      <td>0.176594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>As much as I love  I had the worst experience ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.097861</td>\n",
       "      <td>0.898993</td>\n",
       "      <td>0.003146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>FUCK YOU WTF HAPPEN TO MY 100 POINTS?!</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.098138</td>\n",
       "      <td>0.784290</td>\n",
       "      <td>0.117572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Really   a up charge for soup instead of salad</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.180406</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.210707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>SHIT</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.191904</td>\n",
       "      <td>0.456762</td>\n",
       "      <td>0.351334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>so I went to  again &amp;amp; witnessed a cook gra...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.975790</td>\n",
       "      <td>0.016729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>you're foods shit as</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.175640</td>\n",
       "      <td>0.494225</td>\n",
       "      <td>0.330135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>Sent a complaint to  on May 8th &amp;amp; no ackno...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>0.972946</td>\n",
       "      <td>0.017241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>I've got to repost that last tweet.  I got the...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.307436</td>\n",
       "      <td>0.613832</td>\n",
       "      <td>0.078732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>When  gives you a raw steak and you get sick a...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.274098</td>\n",
       "      <td>0.686440</td>\n",
       "      <td>0.039461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>was wondering why I had 2 wait 15 minutes 2 b...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.019732</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>0.013040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>get your house in order. Stop running your ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.339067</td>\n",
       "      <td>0.508079</td>\n",
       "      <td>0.152854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>When  charges you a $20 order twice 🙃🙃🙃🙃🙃</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.152763</td>\n",
       "      <td>0.739178</td>\n",
       "      <td>0.108059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>Y'all forget my brown rice again me and chang ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.402942</td>\n",
       "      <td>0.479889</td>\n",
       "      <td>0.117169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Thanks  for the amazing waitress in Emily and ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.269169</td>\n",
       "      <td>0.724364</td>\n",
       "      <td>0.006467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>we waited way too long for the steaks so we d...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>0.926958</td>\n",
       "      <td>0.029891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>Marx worked for Coke don't serve Pepsi!! Not ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.149902</td>\n",
       "      <td>0.669301</td>\n",
       "      <td>0.180797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Not bad as brown beer's go has a  slight nut t...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.278309</td>\n",
       "      <td>0.631631</td>\n",
       "      <td>0.090060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>to a  tournament.  of  are raising money for</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.281563</td>\n",
       "      <td>0.550313</td>\n",
       "      <td>0.168124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>Kinda crappy that  posts a Bartending Position...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>0.852137</td>\n",
       "      <td>0.073474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Norman location. Applied and received a call ...</td>\n",
       "      <td>one/two</td>\n",
       "      <td>0.050869</td>\n",
       "      <td>0.928348</td>\n",
       "      <td>0.020783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "60   Too bad  managers were told not to receive our...   \n",
       "71    you guys should send me a lifetime supply as ...   \n",
       "77          Paying workers with pre paid debit cards?    \n",
       "83   Not nice of u  to charge us for playing games ...   \n",
       "84   .’s giant portions of pasta, lasagna &amp; chi...   \n",
       "91   Oh  your customer service is disappointing on ...   \n",
       "93    I got charged for a salad for eating a bite o...   \n",
       "107   nasty ass place I just found a lady bug in my...   \n",
       "108   Suggestion for the  kid's menu: Maybe remove ...   \n",
       "112                   Hands down worst service ever..    \n",
       "120                there's something gross in my food😁   \n",
       "130   When you order online for an 11:00 pickup and...   \n",
       "145   table games are phenomenal, but unannounced f...   \n",
       "152   eating at Carencro Louisiana location. So col...   \n",
       "158  The  by Children's Mercy Park may be the worst...   \n",
       "174   and we waited 13 minutes to get our drink ord...   \n",
       "175                               worst service ever 🙃   \n",
       "178      too bad none of my friends want to come with!   \n",
       "189  When you cant decide what you want at  so you ...   \n",
       "191  Was eating left over  &amp; Hops jumped up on ...   \n",
       "193   \\n piece hot and pieces frozen. .🙈❤\\nThis is ...   \n",
       "196   Terrible experience at Chili's Las Vegas airp...   \n",
       "199   your staff thought this would be an acceptabl...   \n",
       "201   oops I forgot to finish my rant, anyways they...   \n",
       "219  Had to contact red lobster about that horrible...   \n",
       "235  your biscuits suck, your decor sucks, the soap...   \n",
       "239           I just entered a  for a Gift Card to       \n",
       "243  Whenever you want to make me a partner  just l...   \n",
       "244  Arrive at  for lunch. Handed a pager, 5-10 min...   \n",
       "251  Its happened again, Its happened again, Kevin ...   \n",
       "..                                                 ...   \n",
       "497   has touch screens that each individual can pa...   \n",
       "502   why do you give us 3  for a party of 2. You k...   \n",
       "512  Was going to make snarky comment about , but b...   \n",
       "524                      stays messing up to-go orders   \n",
       "538  Beyond angry😡.Not only do i have to wait to ge...   \n",
       "540   i ordered enchiladas with specific sides. Whe...   \n",
       "542   so disappointed in you guys! 💔 you've never m...   \n",
       "551   YO I FOUND 44 NASTY SPIDERS IN UR RESTAURANT ...   \n",
       "552   the typa restaurant to air amy schumers stand...   \n",
       "561                       who in charge of the aux in    \n",
       "568  As much as I love  I had the worst experience ...   \n",
       "569             FUCK YOU WTF HAPPEN TO MY 100 POINTS?!   \n",
       "573     Really   a up charge for soup instead of salad   \n",
       "580                                               SHIT   \n",
       "581  so I went to  again &amp; witnessed a cook gra...   \n",
       "588                               you're foods shit as   \n",
       "593  Sent a complaint to  on May 8th &amp; no ackno...   \n",
       "608  I've got to repost that last tweet.  I got the...   \n",
       "611  When  gives you a raw steak and you get sick a...   \n",
       "614   was wondering why I had 2 wait 15 minutes 2 b...   \n",
       "619     get your house in order. Stop running your ...   \n",
       "621          When  charges you a $20 order twice 🙃🙃🙃🙃🙃   \n",
       "623  Y'all forget my brown rice again me and chang ...   \n",
       "629  Thanks  for the amazing waitress in Emily and ...   \n",
       "630   we waited way too long for the steaks so we d...   \n",
       "634   Marx worked for Coke don't serve Pepsi!! Not ...   \n",
       "653  Not bad as brown beer's go has a  slight nut t...   \n",
       "663   to a  tournament.  of  are raising money for       \n",
       "672  Kinda crappy that  posts a Bartending Position...   \n",
       "673   Norman location. Applied and received a call ...   \n",
       "\n",
       "        pred  \\\n",
       "60   one/two   \n",
       "71   one/two   \n",
       "77   one/two   \n",
       "83   one/two   \n",
       "84   one/two   \n",
       "91   one/two   \n",
       "93   one/two   \n",
       "107  one/two   \n",
       "108  one/two   \n",
       "112  one/two   \n",
       "120  one/two   \n",
       "130  one/two   \n",
       "145  one/two   \n",
       "152  one/two   \n",
       "158  one/two   \n",
       "174  one/two   \n",
       "175  one/two   \n",
       "178  one/two   \n",
       "189  one/two   \n",
       "191  one/two   \n",
       "193  one/two   \n",
       "196  one/two   \n",
       "199  one/two   \n",
       "201  one/two   \n",
       "219  one/two   \n",
       "235  one/two   \n",
       "239  one/two   \n",
       "243  one/two   \n",
       "244  one/two   \n",
       "251  one/two   \n",
       "..       ...   \n",
       "497  one/two   \n",
       "502  one/two   \n",
       "512  one/two   \n",
       "524  one/two   \n",
       "538  one/two   \n",
       "540  one/two   \n",
       "542  one/two   \n",
       "551  one/two   \n",
       "552  one/two   \n",
       "561  one/two   \n",
       "568  one/two   \n",
       "569  one/two   \n",
       "573  one/two   \n",
       "580  one/two   \n",
       "581  one/two   \n",
       "588  one/two   \n",
       "593  one/two   \n",
       "608  one/two   \n",
       "611  one/two   \n",
       "614  one/two   \n",
       "619  one/two   \n",
       "621  one/two   \n",
       "623  one/two   \n",
       "629  one/two   \n",
       "630  one/two   \n",
       "634  one/two   \n",
       "653  one/two   \n",
       "663  one/two   \n",
       "672  one/two   \n",
       "673  one/two   \n",
       "\n",
       "     prob_4/5  \\\n",
       "60   0.003580   \n",
       "71   0.277998   \n",
       "77   0.051944   \n",
       "83   0.140991   \n",
       "84   0.369090   \n",
       "91   0.114815   \n",
       "93   0.098855   \n",
       "107  0.001286   \n",
       "108  0.158861   \n",
       "112  0.039142   \n",
       "120  0.273510   \n",
       "130  0.037292   \n",
       "145  0.426427   \n",
       "152  0.006095   \n",
       "158  0.359598   \n",
       "174  0.000649   \n",
       "175  0.045985   \n",
       "178  0.223874   \n",
       "189  0.277673   \n",
       "191  0.152213   \n",
       "193  0.206504   \n",
       "196  0.008358   \n",
       "199  0.164441   \n",
       "201  0.399334   \n",
       "219  0.001493   \n",
       "235  0.017316   \n",
       "239  0.316505   \n",
       "243  0.416631   \n",
       "244  0.110134   \n",
       "251  0.311300   \n",
       "..        ...   \n",
       "497  0.114376   \n",
       "502  0.192659   \n",
       "512  0.281610   \n",
       "524  0.192192   \n",
       "538  0.066133   \n",
       "540  0.245400   \n",
       "542  0.000635   \n",
       "551  0.003486   \n",
       "552  0.396477   \n",
       "561  0.309455   \n",
       "568  0.097861   \n",
       "569  0.098138   \n",
       "573  0.180406   \n",
       "580  0.191904   \n",
       "581  0.007481   \n",
       "588  0.175640   \n",
       "593  0.009813   \n",
       "608  0.307436   \n",
       "611  0.274098   \n",
       "614  0.019732   \n",
       "619  0.339067   \n",
       "621  0.152763   \n",
       "623  0.402942   \n",
       "629  0.269169   \n",
       "630  0.043150   \n",
       "634  0.149902   \n",
       "653  0.278309   \n",
       "663  0.281563   \n",
       "672  0.074389   \n",
       "673  0.050869   \n",
       "\n",
       "     prob_1/2  \\\n",
       "60   0.989221   \n",
       "71   0.642817   \n",
       "77   0.837278   \n",
       "83   0.553386   \n",
       "84   0.374450   \n",
       "91   0.879715   \n",
       "93   0.773735   \n",
       "107  0.986581   \n",
       "108  0.797564   \n",
       "112  0.905843   \n",
       "120  0.596422   \n",
       "130  0.946924   \n",
       "145  0.538117   \n",
       "152  0.950593   \n",
       "158  0.549990   \n",
       "174  0.986108   \n",
       "175  0.893782   \n",
       "178  0.696361   \n",
       "189  0.536540   \n",
       "191  0.683653   \n",
       "193  0.650939   \n",
       "196  0.990475   \n",
       "199  0.604505   \n",
       "201  0.497711   \n",
       "219  0.967299   \n",
       "235  0.883840   \n",
       "239  0.522541   \n",
       "243  0.572696   \n",
       "244  0.866937   \n",
       "251  0.631659   \n",
       "..        ...   \n",
       "497  0.673000   \n",
       "502  0.551770   \n",
       "512  0.601295   \n",
       "524  0.746653   \n",
       "538  0.618964   \n",
       "540  0.568754   \n",
       "542  0.995637   \n",
       "551  0.986109   \n",
       "552  0.462611   \n",
       "561  0.513951   \n",
       "568  0.898993   \n",
       "569  0.784290   \n",
       "573  0.608888   \n",
       "580  0.456762   \n",
       "581  0.975790   \n",
       "588  0.494225   \n",
       "593  0.972946   \n",
       "608  0.613832   \n",
       "611  0.686440   \n",
       "614  0.967228   \n",
       "619  0.508079   \n",
       "621  0.739178   \n",
       "623  0.479889   \n",
       "629  0.724364   \n",
       "630  0.926958   \n",
       "634  0.669301   \n",
       "653  0.631631   \n",
       "663  0.550313   \n",
       "672  0.852137   \n",
       "673  0.928348   \n",
       "\n",
       "       prob_3  \n",
       "60   0.007199  \n",
       "71   0.079185  \n",
       "77   0.110778  \n",
       "83   0.305624  \n",
       "84   0.256460  \n",
       "91   0.005469  \n",
       "93   0.127410  \n",
       "107  0.012133  \n",
       "108  0.043575  \n",
       "112  0.055015  \n",
       "120  0.130068  \n",
       "130  0.015784  \n",
       "145  0.035457  \n",
       "152  0.043313  \n",
       "158  0.090412  \n",
       "174  0.013244  \n",
       "175  0.060233  \n",
       "178  0.079765  \n",
       "189  0.185787  \n",
       "191  0.164134  \n",
       "193  0.142557  \n",
       "196  0.001167  \n",
       "199  0.231054  \n",
       "201  0.102955  \n",
       "219  0.031208  \n",
       "235  0.098844  \n",
       "239  0.160954  \n",
       "243  0.010673  \n",
       "244  0.022929  \n",
       "251  0.057041  \n",
       "..        ...  \n",
       "497  0.212624  \n",
       "502  0.255571  \n",
       "512  0.117095  \n",
       "524  0.061155  \n",
       "538  0.314903  \n",
       "540  0.185846  \n",
       "542  0.003728  \n",
       "551  0.010405  \n",
       "552  0.140912  \n",
       "561  0.176594  \n",
       "568  0.003146  \n",
       "569  0.117572  \n",
       "573  0.210707  \n",
       "580  0.351334  \n",
       "581  0.016729  \n",
       "588  0.330135  \n",
       "593  0.017241  \n",
       "608  0.078732  \n",
       "611  0.039461  \n",
       "614  0.013040  \n",
       "619  0.152854  \n",
       "621  0.108059  \n",
       "623  0.117169  \n",
       "629  0.006467  \n",
       "630  0.029891  \n",
       "634  0.180797  \n",
       "653  0.090060  \n",
       "663  0.168124  \n",
       "672  0.073474  \n",
       "673  0.020783  \n",
       "\n",
       "[96 rows x 5 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df[tweet_df['pred'] == 'one/two'].iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
