{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Import needed modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English ##Note you'll need to install Spacy and download its dependencies\n",
    "parser = English()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = train(words(open('C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/bigtext.txt').read()))\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "#     mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "#     text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    text = re.sub('[^a-zA-Z0-9 ]','',text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "#     text = correct(text)\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(str(TextBlob(sample).correct()))\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/sentence_raw.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and munge munge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df[['sentence','category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2363: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.ix._setitem_with_indexer(indexer, value)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2343: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_array(key, value)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "##Collapse some\n",
    "df2[df2['category']==\"wait\"] = \"service\" \n",
    "df2[df2['category']==\"value\"] = \"overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food        0.414732\n",
       "overall     0.263298\n",
       "others      0.138862\n",
       "service     0.123872\n",
       "ambiance    0.059236\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.category.value_counts()/df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2698: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "df2.sentence = df2.sentence.apply(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "\n",
    "hashvectorizer = HashingVectorizer(ngram_range=(1,3),tokenizer=tokenizeText)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3),min_df=3, max_features=5000,tokenizer=tokenizeText)\n",
    "tfvectorizer = TfidfVectorizer(ngram_range=(1,3),min_df = 3,max_features=5000,tokenizer=tokenizeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Hashing Vectorizer (slow)\n",
    "hashfeature = hashvectorizer.fit_transform(df2.sentence)\n",
    "hashfeature = pickle.dumps(hashfeature)\n",
    "featuredf_hash = pd.DataFrame(hashfeature.A)\n",
    "# df3_hash = pd.concat((df2,featuredf_hash),axis=1)\n",
    "# print(df3_hash.info())\n",
    "# df3_hash.to_pickle(\"df3_hash.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24816 entries, 0 to 24815\n",
      "Columns: 5002 entries, sentence to zucchini\n",
      "dtypes: int64(5000), object(2)\n",
      "memory usage: 947.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Gets the count of each word in each sentence (Count Vectorizer)\n",
    "countfeature = vectorizer.fit_transform(df2.sentence)\n",
    "# lsa_count = TruncatedSVD(n_components=50,n_iter=100).fit_transform(countfeature)\n",
    "# lsa_count_df = pd.DataFrame(lsa_count)\n",
    "featuredf_count = pd.DataFrame(countfeature.A, columns=vectorizer.get_feature_names())\n",
    "df3_count = pd.concat((df2,featuredf_count),axis=1)\n",
    "# df3_lsa_count = pd.concat((df2,lsa_count_df),axis=1)\n",
    "print(df3_count.info())\n",
    "# print(df3_lsa_count.info())\n",
    "df3_count.to_pickle(\"df3_count.pkl\")\n",
    "# df3_lsa_count.to_pickle(\"df3_lsa_count.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24816 entries, 0 to 24815\n",
      "Columns: 5002 entries, sentence to zucchini\n",
      "dtypes: float64(5000), object(2)\n",
      "memory usage: 947.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##TFIDF Vectorizer (slow)\n",
    "tffeature = tfvectorizer.fit_transform(df2.sentence)\n",
    "featuredf_tf = pd.DataFrame(tffeature.A, columns=tfvectorizer.get_feature_names())\n",
    "# lsa_tf = TruncatedSVD(n_components=50,n_iter=100).fit_transform(tffeature)\n",
    "# lsa_tf_df = pd.DataFrame(lsa_tf)\n",
    "df3_tf = pd.concat((df2,featuredf_tf),axis=1)\n",
    "# df3_lsa_tf = pd.concat((df2,lsa_tf_df),axis=1)\n",
    "print(df3_tf.info())\n",
    "# print(df3_lsa_tf.info())\n",
    "df3_tf.to_pickle(\"df3_tf.pkl\")\n",
    "# df3_lsa_tf.to_pickle(\"df3_lsa_tf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Pickle vocab\n",
    "count_dict = vectorizer.get_feature_names()\n",
    "tf_dict = tfvectorizer.get_feature_names()\n",
    "count_dict = pickle.dumps(count_dict)\n",
    "tf_dict = pickle.dumps(tf_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model and in-training-set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split \n",
    "df3_count = pd.read_pickle(\"df3_count.pkl\")\n",
    "df3_tf = pd.read_pickle(\"df3_tf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extrasample_df = df3_count[df3_count.category.iloc[:,0]=='ambiance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expanded_df = pd.concat((df3_count,extrasample_df),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expanded_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food        10292\n",
       "overall      6534\n",
       "others       3446\n",
       "service      3074\n",
       "ambiance     2940\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df.iloc[:,1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Split into train and test at 75/25\n",
    "train, test = train_test_split(expanded_df.values,test_size = 0.25, random_state=1)\n",
    "\n",
    "##Split X & Y\n",
    "X_train = train[:,2:]\n",
    "Y_train = train[:,1]\n",
    "X_test = test[:,2:]\n",
    "Y_test = test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy:  0.627358490566\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   ambiance       0.65      0.64      0.64       726\n",
      "       food       0.71      0.77      0.74      2570\n",
      "     others       0.45      0.35      0.39       897\n",
      "    overall       0.55      0.54      0.54      1656\n",
      "    service       0.63      0.65      0.64       723\n",
      "\n",
      "avg / total       0.62      0.63      0.62      6572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB().fit(X_train,Y_train)\n",
    "nb_Y_pred = nb.predict(X_test)\n",
    "print(\"NB Accuracy: \",np.mean(nb_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,nb_Y_pred))\n",
    "\n",
    "## Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(multi_class='multinomial',solver='newton-cg').fit(X_train,Y_train)\n",
    "log_Y_pred = log.predict(X_test)\n",
    "print(\"Logistc Accuracy: \",np.mean(log_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,log_Y_pred))\n",
    "\n",
    "##KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5).fit(X_train,Y_train)\n",
    "knn_Y_pred = knn.predict(X_test)\n",
    "print(neigh,\"KNN Accuracy: \",np.mean(knn_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,knn_Y_pred))\n",
    "\n",
    "## Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "svcl = SVC(kernel='linear').fit(X_train,Y_train)\n",
    "svcl_Y_pred = svcl.predict(X_test)\n",
    "print(\"SVC Linear Accuracy: \",np.mean(svcl_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,svcl_Y_pred))\n",
    "\n",
    "## RBF SVC\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# sgd = SGDClassifier(loss='perceptron',penalty='elasticnet',l1_ratio=0.5).fit(X_train,Y_train)\n",
    "# sgd_Y_pred = sgd.predict(X_test)\n",
    "# print(\"SGD Perceptron Accuracy: \",np.mean(sgd_Y_pred == np.array(Y_test)))\n",
    "# print(classification_report(Y_test,sgd_Y_pred))\n",
    "\n",
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "dt_Y_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \",np.mean(dt_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,dt_Y_pred))\n",
    "\n",
    "## Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train,Y_train)\n",
    "rf_Y_pred = rf.predict(X_test)\n",
    "print(\"Random Forests Accuracy: \",np.mean(rf_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,rf_Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Split into train and test at 75/25\n",
    "train, test = train_test_split(df3_count.values,test_size = 0.25, random_state=1)\n",
    "\n",
    "##Split X & Y\n",
    "X_train = train[:,2:]\n",
    "Y_train = train[:,1]\n",
    "X_test = test[:,2:]\n",
    "Y_test = test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB().fit(X_train,Y_train)\n",
    "nb_Y_pred = nb.predict(X_test)\n",
    "print(\"NB Accuracy: \",np.mean(nb_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,nb_Y_pred))\n",
    "\n",
    "## Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(multi_class='multinomial',solver='newton-cg').fit(X_train,Y_train)\n",
    "log_Y_pred = log.predict(X_test)\n",
    "print(\"Logistc Accuracy: \",np.mean(log_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,log_Y_pred))\n",
    "\n",
    "##KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5).fit(X_train,Y_train)\n",
    "knn_Y_pred = knn.predict(X_test)\n",
    "print(neigh,\"KNN Accuracy: \",np.mean(knn_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,knn_Y_pred))\n",
    "\n",
    "## Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "svcl = SVC(kernel='linear').fit(X_train,Y_train)\n",
    "svcl_Y_pred = svcl.predict(X_test)\n",
    "print(\"SVC Linear Accuracy: \",np.mean(svcl_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,svcl_Y_pred))\n",
    "\n",
    "## RBF SVC\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# sgd = SGDClassifier(loss='perceptron',penalty='elasticnet',l1_ratio=0.5).fit(X_train,Y_train)\n",
    "# sgd_Y_pred = sgd.predict(X_test)\n",
    "# print(\"SGD Perceptron Accuracy: \",np.mean(sgd_Y_pred == np.array(Y_test)))\n",
    "# print(classification_report(Y_test,sgd_Y_pred))\n",
    "\n",
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "dt_Y_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \",np.mean(dt_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,dt_Y_pred))\n",
    "\n",
    "## Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train,Y_train)\n",
    "rf_Y_pred = rf.predict(X_test)\n",
    "print(\"Random Forests Accuracy: \",np.mean(rf_Y_pred == np.array(Y_test)))\n",
    "print(classification_report(Y_test,rf_Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Pickle models\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(nb,'nb_sentence.pkl')\n",
    "joblib.dump(knn,'knn_sentence.pkl')\n",
    "joblib.dump(log,'log_sentence.pkl')\n",
    "joblib.dump(svcl,'svcl_sentence.pkl')\n",
    "joblib.dump(sgd,'sgd_sentence.pkl')\n",
    "joblib.dump(dt,'dt_sentence.pkl')\n",
    "joblib.dump(rf,'rf_sentence.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# food = 1\n",
    "# service = 2\n",
    "# ambiance = 3\n",
    "# overall = 4\n",
    "# others = 5\n",
    "\n",
    "def integerize(x):\n",
    "    if x == 'food':\n",
    "        return 1\n",
    "    elif x == 'service':\n",
    "        return 2\n",
    "    elif x == 'ambiance':\n",
    "        return 3\n",
    "    elif x == 'overall':\n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = df3_tf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4.category = df3_tf.category.apply(integerize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_nn, test_nn = train_test_split(df4.values,test_size = 0.25)\n",
    "X_train_nn = train_nn[:,2:]\n",
    "Y_train_nn = train_nn[:,1]\n",
    "X_test_nn = test_nn[:,2:]\n",
    "Y_test_nn = test_nn[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Tanh\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.001,\n",
    "    n_iter=50,\n",
    "    loss_type='mcc')\n",
    "nn.fit(df4.iloc[:,2:], df4.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = nn.predict(df4.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"NN: \",np.mean(y_pred == np.array(df4.iloc[:,1])))\n",
    "print(classification_report(df4.iloc[:,1],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = df2.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_list = tfvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dict = dict.fromkeys(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfvectorizer2 = TfidfVectorizer(ngram_range=(1,3),min_df = 3,tokenizer=tokenizeText,sublinear_tf=True,vocabulary=vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check = tfvectorizer.transform(new_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x208 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 286 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
